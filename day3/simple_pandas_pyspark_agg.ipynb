 {
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63U4qGDjy8qK"
      },
      "source": [
        "# Exploring Spark with Pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5SHATmly8qM"
      },
      "source": [
        "Using pandas examples, convert the analysis to pyspark. This is useful if you discover your data grows too large for your tooling.\n",
        "\n",
        "The purpose of this notebook is to familiarise yourself you the pyspark API. You are welcome to use the R version of this if you wish. As long as you are able to obtain the correct results. We will be using python in this notebook as it is quite widely used through data science and the community is very large.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dVdG6Hhy8qN"
      },
      "source": [
        "#### Firstly, let's get our spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84AZTTNKy8qN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "spark = SparkSession.builder.appName('panda-and-spark').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0vC0ugTy8qO"
      },
      "source": [
        "### Overview\n",
        "\n",
        "\n",
        "* Joining two dataframes/data sets\n",
        "* Simple aggregations\n",
        "* Persisting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF4uKuQyy8qO"
      },
      "source": [
        "#### JOIN: Pandas\n",
        "\n",
        "We won't use this more in this notebook, but observe how the joins work.\n",
        "\n",
        "We what happens if you change from the default inner join to outer joins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqUZ34cwy8qP"
      },
      "outputs": [],
      "source": [
        "customer_raw = [(1, 'bob', 3462543658686),\n",
        "           (2, 'rob', 9087567565439),\n",
        "           (3, 'tim', 5436586999467),\n",
        "           (4, 'tom', 8349756853250)]\n",
        "\n",
        "customer_cols = ['id', 'name', 'credit_card_number']\n",
        "\n",
        "\n",
        "\n",
        "orders_raw = [(1, 'ketchup', 'bob', 1.20),\n",
        "           (2, 'rutabaga', 'bob', 3.35),\n",
        "           (3, 'fake vegan meat', 'rob', 13.99),\n",
        "           (4, 'cheesey poofs', 'tim', 3.99),\n",
        "           (5, 'ice cream', 'tim', 4.95),\n",
        "           (6, 'protein powder', 'tom', 49.95)]\n",
        "\n",
        "orders_cols = ['id', 'product_name', 'customer', 'price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StwR60UCy8qP"
      },
      "outputs": [],
      "source": [
        "customer_df = pd.DataFrame(customer_raw, columns=customer_cols)\n",
        "orders_df = pd.DataFrame(orders_raw, columns=orders_cols)\n",
        "\n",
        "customer_df\n",
        "\n",
        "joined_df = pd.merge(customer_df, orders_df, how='inner', left_on='name', right_on='customer')\n",
        "joined_df\n",
        "\n",
        "## For self study. What happens if (4, 'tom', 8349756853250) in valuesA becomes (4, 'tod', 8349756853250)\n",
        "## How do the results change?\n",
        "## More sensibly; what if customers have not made any orders but we still require them in the result set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set46Oxqy8qP"
      },
      "source": [
        "#### JOIN: Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEpcGnNYy8qQ"
      },
      "outputs": [],
      "source": [
        "customersDF = spark.createDataFrame(customer_raw, customer_cols)\n",
        "\n",
        "ordersDF = spark.createDataFrame(orders_raw, orders_cols)\n",
        "\n",
        "# Show tables\n",
        "customersDF.show()\n",
        "ordersDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Lca96cy8qQ"
      },
      "outputs": [],
      "source": [
        "joinedDF = customersDF.join(ordersDF, customersDF.name == ordersDF.customer)\n",
        "joinedDF.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bFT5Qujy8qQ"
      },
      "source": [
        "## Simple Aggregations\n",
        "\n",
        "Now let's explore simple aggregations. You will be using these often when doing exploratory work in big data. Remember, the intention here is that you grow familiar with the way the API works, and how to translate inquiries into that API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RbIPRUMy8qQ"
      },
      "source": [
        "> _How much did each person spend?_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MScxoSKHy8qQ"
      },
      "outputs": [],
      "source": [
        "joined_df.groupby('name').agg({\"price\": [\"sum\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQYcpx-by8qR"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as f\n",
        "\n",
        "joinedDF.groupby('name').agg(f.sum('price').alias('total')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZYEmfuey8qR"
      },
      "source": [
        "Let's use bigger data\n",
        "  * NYC crash data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDW0xMXFy8qR"
      },
      "outputs": [],
      "source": [
        "# save to the filesystem to prevent another load\n",
        "# ! curl -o rows.csv https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRq1q_O5y8qR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "nyc_df = pd.read_csv('rows.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwizMUE6y8qR"
      },
      "outputs": [],
      "source": [
        "# number or rows\n",
        "\n",
        "print(len(nyc_df))\n",
        "\n",
        "# this is quite large so we will work with a sample while we experiment in pandas as least."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCNmjziSy8qR"
      },
      "source": [
        "We'll take a random sample at 20% of the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q11YkkRRy8qR"
      },
      "outputs": [],
      "source": [
        "nyc_small = nyc_df.sample(frac=0.2, replace=False, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUSIt_Uly8qR"
      },
      "outputs": [],
      "source": [
        "# we are also going to limit the columns to those we are going to work with\n",
        "\n",
        "nyc_small = nyc_small[['CRASH DATE', 'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "                       'BOROUGH', 'VEHICLE TYPE CODE 1',\n",
        "                       'NUMBER OF PERSONS INJURED']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZqsDWVqy8qR"
      },
      "outputs": [],
      "source": [
        "nyc_small.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y3bM1LMy8qR"
      },
      "source": [
        "Now, let's create the pyspark dataframe. Now we two frames with the same content\n",
        "  * nyc_small: pandas\n",
        "  * sdf_small: pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJiRpDQKy8qR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "\n",
        "# there are nan's in the frame with strings, and spark can't 'infer the schema', so we have to help it out\n",
        "# by replacing them with empty strings and forcing the column to be a string\n",
        "\n",
        "sdf_small = SQLContext(spark).createDataFrame(nyc_small.fillna('').astype('str'))\n",
        "\n",
        "\n",
        "# Lets check the schema quickly\n",
        "\n",
        "print(sdf_small.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB4YE7cUy8qS"
      },
      "source": [
        "# Questions\n",
        "\n",
        "Answer the following questions by porting the pandas code to the Spark API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7aaHg0Jy8qS"
      },
      "source": [
        "\n",
        "# Question 1\n",
        "\n",
        "\n",
        "> On what day do most crashes occcur?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O3sDvNzy8qS"
      },
      "outputs": [],
      "source": [
        "# Pandas\n",
        "nyc_small.groupby('CRASH DATE')['CRASH DATE'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHaZJ1pky8qS"
      },
      "outputs": [],
      "source": [
        "### Spark?\n",
        "crashes_by_date = sdf_small.groupBy(\"CRASH DATE\").count().orderBy(\"count\", ascending=False)\n",
        "crashes_by_date.show(5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8keAFkyy8qS"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "> _Where do most crashes occur?_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Euc9DsEy8qS"
      },
      "outputs": [],
      "source": [
        "nyc_small.groupby('BOROUGH')['BOROUGH'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2D-CDS7y8qS"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "crashes_by_borough = sdf_small.groupBy(\"BOROUGH\").count().orderBy(\"count\", ascending=False)\n",
        "crashes_by_borough.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQKFAqw8y8qS"
      },
      "source": [
        " # Question 3\n",
        "\n",
        " > What is the most common cause of accident in 'QUEENS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELpIiVGy8qS"
      },
      "outputs": [],
      "source": [
        "nyc_small[(nyc_small.BOROUGH == 'QUEENS')]['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()\n",
        "\n",
        "# you can also use a group by (to avoid the pandas value_counts function)\n",
        "\n",
        "nyc_small[(nyc_small.BOROUGH == 'QUEENS')].groupby(\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1'\n",
        ")['CONTRIBUTING FACTOR VEHICLE 1'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIFBeGTyy8qS"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "queens_accidents = sdf_small.filter(sdf_small[\"BOROUGH\"] == \"QUEENS\")\n",
        "common_causes_queens = queens_accidents.groupBy(\"CONTRIBUTING FACTOR VEHICLE 1\").count().orderBy(\"count\", ascending=False)\n",
        "common_causes_queens.show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGs_8eKy8qS"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsM5hIpFy8qS"
      },
      "source": [
        "> _What is the average number or injuries for specific cars driving in specific suburbs_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I0ZL4J6y8qT"
      },
      "outputs": [],
      "source": [
        "nyc_small.groupby(['VEHICLE TYPE CODE 1', 'BOROUGH'])['NUMBER OF PERSONS INJURED'].mean().sort_values(ascending=False).head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rl7Sp_Iy8qT"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "average_injuries = sdf_small.groupBy(\"VEHICLE TYPE CODE 1\", \"BOROUGH\")\\\n",
        "                            .agg(f.avg(\"NUMBER OF PERSONS INJURED\").alias(\"avg_injuries\"))\\\n",
        "                            .orderBy(\"avg_injuries\", ascending=False)\n",
        "average_injuries.show(3)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}