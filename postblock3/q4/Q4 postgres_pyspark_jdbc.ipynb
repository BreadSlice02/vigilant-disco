{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QbPsmEf6ljt"
   },
   "source": [
    "# Purpose\n",
    "\n",
    "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
    "\n",
    "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-RHL4bg4u0_"
   },
   "source": [
    "# Setup\n",
    "\n",
    "## PostgreSQL\n",
    "\n",
    "Firstly, let's install postgres in the this Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhmGVh22JcNo",
    "outputId": "de134606-3fd2-4cb3-badf-18c0f03e71c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
      "  logrotate netbase postgresql-14 postgresql-client-14\n",
      "  postgresql-client-common postgresql-common ssl-cert sysstat\n",
      "Suggested packages:\n",
      "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
      "The following NEW packages will be installed:\n",
      "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
      "  logrotate netbase postgresql postgresql-14 postgresql-client-14\n",
      "  postgresql-client-common postgresql-common postgresql-contrib ssl-cert\n",
      "  sysstat\n",
      "0 upgraded, 14 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 18.4 MB of archives.\n",
      "After this operation, 51.7 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-xs-perl amd64 4.030-1build3 [87.2 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.13-0ubuntu0.22.04.1 [1,225 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.13-0ubuntu0.22.04.1 [16.2 MB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
      "Fetched 18.4 MB in 1s (28.7 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 14.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package logrotate.\n",
      "(Reading database ... 123622 files and directories currently installed.)\n",
      "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
      "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
      "Selecting previously unselected package netbase.\n",
      "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
      "Unpacking netbase (6.3) ...\n",
      "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
      "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
      "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
      "Selecting previously unselected package libjson-perl.\n",
      "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
      "Unpacking libjson-perl (4.04000-1) ...\n",
      "Selecting previously unselected package libtypes-serialiser-perl.\n",
      "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
      "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
      "Selecting previously unselected package libjson-xs-perl.\n",
      "Preparing to unpack .../05-libjson-xs-perl_4.030-1build3_amd64.deb ...\n",
      "Unpacking libjson-xs-perl (4.030-1build3) ...\n",
      "Selecting previously unselected package postgresql-client-common.\n",
      "Preparing to unpack .../06-postgresql-client-common_238_all.deb ...\n",
      "Unpacking postgresql-client-common (238) ...\n",
      "Selecting previously unselected package postgresql-client-14.\n",
      "Preparing to unpack .../07-postgresql-client-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package ssl-cert.\n",
      "Preparing to unpack .../08-ssl-cert_1.1.2_all.deb ...\n",
      "Unpacking ssl-cert (1.1.2) ...\n",
      "Selecting previously unselected package postgresql-common.\n",
      "Preparing to unpack .../09-postgresql-common_238_all.deb ...\n",
      "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
      "Unpacking postgresql-common (238) ...\n",
      "Selecting previously unselected package postgresql-14.\n",
      "Preparing to unpack .../10-postgresql-14_14.13-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package postgresql.\n",
      "Preparing to unpack .../11-postgresql_14+238_all.deb ...\n",
      "Unpacking postgresql (14+238) ...\n",
      "Selecting previously unselected package postgresql-contrib.\n",
      "Preparing to unpack .../12-postgresql-contrib_14+238_all.deb ...\n",
      "Unpacking postgresql-contrib (14+238) ...\n",
      "Selecting previously unselected package sysstat.\n",
      "Preparing to unpack .../13-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
      "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
      "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
      "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer → /lib/systemd/system/logrotate.timer.\n",
      "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
      "Setting up ssl-cert (1.1.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
      "Setting up libjson-perl (4.04000-1) ...\n",
      "Setting up netbase (6.3) ...\n",
      "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\n",
      "Creating config file /etc/default/sysstat with new version\n",
      "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
      "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer → /lib/systemd/system/sysstat-collect.timer.\n",
      "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer → /lib/systemd/system/sysstat-summary.timer.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
      "Setting up postgresql-client-common (238) ...\n",
      "Setting up libjson-xs-perl (4.030-1build3) ...\n",
      "Setting up postgresql-client-14 (14.13-0ubuntu0.22.04.1) ...\n",
      "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
      "Setting up postgresql-common (238) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Adding user postgres to group ssl-cert\n",
      "\n",
      "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
      "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
      "Removing obsolete dictionary files:\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
      "Setting up postgresql-14 (14.13-0ubuntu0.22.04.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Creating new PostgreSQL cluster 14/main ...\n",
      "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
      "The files belonging to this database system will be owned by user \"postgres\".\n",
      "This user must also own the server process.\n",
      "\n",
      "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
      "The default database encoding has accordingly been set to \"UTF8\".\n",
      "The default text search configuration will be set to \"english\".\n",
      "\n",
      "Data page checksums are disabled.\n",
      "\n",
      "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
      "creating subdirectories ... ok\n",
      "selecting dynamic shared memory implementation ... posix\n",
      "selecting default max_connections ... 100\n",
      "selecting default shared_buffers ... 128MB\n",
      "selecting default time zone ... Etc/UTC\n",
      "creating configuration files ... ok\n",
      "running bootstrap script ... ok\n",
      "performing post-bootstrap initialization ... ok\n",
      "syncing data to disk ... ok\n",
      "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up postgresql-contrib (14+238) ...\n",
      "Setting up postgresql (14+238) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install postgresql postgresql-contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajhL0Z_-KK8r",
    "outputId": "7966c4b1-965c-47b6-ce9f-e9dc79c6d6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Starting PostgreSQL 14 database server\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "!service postgresql start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P48P8Vt6Fm9"
   },
   "source": [
    "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b25UVuzVNdKs",
    "outputId": "48de264e-e696-4848-d08e-ff7864307cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALTER ROLE\n"
     ]
    }
   ],
   "source": [
    "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW1kucySWAKv"
   },
   "source": [
    "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
    "\n",
    "We'll use the notebook magic `%end`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as0Zs9kL6PY0",
    "outputId": "038d1ba9-60c1-45d3-eebe-0bdb998c85b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PGPASSWORD=test\n"
     ]
    }
   ],
   "source": [
    "%env PGPASSWORD=test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGqYbg366efu"
   },
   "source": [
    "## Pagila\n",
    "\n",
    "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qICjoP_dKS8G",
    "outputId": "c41e011f-04a9-4b13-c27c-570e00ea7536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pagila'...\n",
      "remote: Enumerating objects: 94, done.\u001b[K\n",
      "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
      "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
      "remote: Total 94 (delta 47), reused 85 (delta 42), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (94/94), 2.91 MiB | 13.05 MiB/s, done.\n",
      "Resolving deltas: 100% (47/47), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/spatialedge-ai/pagila.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYHVKYqSMthy",
    "outputId": "8592e47e-36c5-4ce0-b952-b63d8df6f97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE DATABASE\n"
     ]
    }
   ],
   "source": [
    "!psql -h localhost -U postgres -c \"create database pagila\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfgNogz3MSq_",
    "outputId": "ba1d7858-dc69-4711-d410-c5fd1806411e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      " set_config \n",
      "------------\n",
      " \n",
      "(1 row)\n",
      "\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "CREATE TYPE\n",
      "ALTER TYPE\n",
      "CREATE DOMAIN\n",
      "ALTER DOMAIN\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "SET\n",
      "SET\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE FUNCTION\n",
      "ALTER FUNCTION\n",
      "CREATE AGGREGATE\n",
      "ALTER AGGREGATE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE SEQUENCE\n",
      "ALTER TABLE\n",
      "CREATE TABLE\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "CREATE VIEW\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "CREATE INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "ALTER INDEX\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "CREATE TRIGGER\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n",
      "ALTER TABLE\n"
     ]
    }
   ],
   "source": [
    "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zpqaYNZPABo",
    "outputId": "4c872f63-a108-4fea-ba9f-a234df5d6c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      " set_config \n",
      "------------\n",
      " \n",
      "(1 row)\n",
      "\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "SET\n",
      "COPY 200\n",
      "COPY 109\n",
      "COPY 600\n",
      "COPY 603\n",
      "COPY 16\n",
      "COPY 2\n",
      "COPY 599\n",
      "COPY 6\n",
      "COPY 1000\n",
      "COPY 5462\n",
      "COPY 1000\n",
      "COPY 4581\n",
      "COPY 2\n",
      "COPY 16044\n",
      "COPY 1157\n",
      "COPY 2312\n",
      "COPY 5644\n",
      "COPY 6754\n",
      "COPY 182\n",
      "COPY 0\n",
      " setval \n",
      "--------\n",
      "    200\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "    605\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "     16\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "    600\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "    109\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "    599\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "   1000\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "   4581\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "      6\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "  32098\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "  16049\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "      2\n",
      "(1 row)\n",
      "\n",
      " setval \n",
      "--------\n",
      "      2\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M0a4GiI6yyr"
   },
   "source": [
    "## PySpark Setup\n",
    "\n",
    "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCiCzTg-Jx2Q",
    "outputId": "28c18b00-02ca-4f75-c306-2cb1e049bf3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-23 06:56:05--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1046274 (1022K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.5.0.jar’\n",
      "\n",
      "postgresql-42.5.0.j 100%[===================>]   1022K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-10-23 06:56:06 (8.68 MB/s) - ‘postgresql-42.5.0.jar’ saved [1046274/1046274]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2BQsxrwZBhWc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "SPARKVERSION='3.2.1'\n",
    "HADOOPVERSION='3.2'\n",
    "pwd=os.getcwd()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
    "\n",
    "# print(os.environ['SPARK_HOME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1owkTgHVBuix",
    "outputId": "9199c1f4-6058-400f-ce99-082dc6d55de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "--2024-10-23 06:56:32--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
      "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 300971569 (287M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.2.1-bin-hadoop3.2.tgz’\n",
      "\n",
      "spark-3.2.1-bin-had 100%[===================>] 287.03M   519KB/s    in 7m 41s  \n",
      "\n",
      "2024-10-23 07:04:13 (638 KB/s) - ‘spark-3.2.1-bin-hadoop3.2.tgz’ saved [300971569/300971569]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
    "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ighjc_WdUNgC"
   },
   "outputs": [],
   "source": [
    "!cp postgresql-42.5.0.jar spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCIQhdSYC5uh",
    "outputId": "e8271755-4467-47cf-c4df-a8d66df18603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reMhwdxpCz05",
    "outputId": "983fe06e-24ec-4aa1-fb52-4c5f7c5cd7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql-42.2.5.jar\n",
      "env: PYARROW_IGNORE_TIMEZONE=1\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "# get a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars\",\n",
    "                                                       \"postgresql-42.2.5.jar\").config(\n",
    "                                                          \"spark.driver.extraClassPath\",\n",
    "                                                          f\"spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}/jars\"\n",
    "                                                       ).getOrCreate()\n",
    "print(spark.conf.get('spark.jars'))\n",
    "\n",
    "%env PYARROW_IGNORE_TIMEZONE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqG_Hk4YXuC7"
   },
   "source": [
    "# Questions\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnrQk09jQyaJ",
    "outputId": "367e4ca6-4853-4813-e830-1382d92b20dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- address_id: integer (nullable = true)\n",
      " |-- activebool: boolean (nullable = true)\n",
      " |-- create_date: date (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      " |-- active: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the postgresql connection properties\n",
    "jdbcHostname = \"localhost\"\n",
    "jdbcPort = 5432\n",
    "jdbcDatabase = \"pagila\"\n",
    "jdbcUrl = f\"jdbc:postgresql://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}\"\n",
    "connectionProperties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"test\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# read the customer table into a pyspark DataFrame\n",
    "customer_df = spark.read.jdbc(\n",
    "    url=jdbcUrl,\n",
    "    table=\"customer\",\n",
    "    properties=connectionProperties\n",
    ")\n",
    "\n",
    "# print the schema\n",
    "customer_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXhnjaylCFI1"
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTGwAFhYpanl",
    "outputId": "d00ff614-57ce-4d4e-fcdb-007fa0f99d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|unique_email_count|\n",
      "+------------------+\n",
      "|               599|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the dataframe as a temporary view\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "# use spark sql to compute the number of unique email addresses\n",
    "unique_emails = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT email) AS unique_email_count\n",
    "    FROM customer\n",
    "\"\"\")\n",
    "\n",
    "# show\n",
    "unique_emails.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg7To_5dCRGb"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Repeat this calculation using only the Dataframe API and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTO78anmCa37",
    "outputId": "eba21861-b860-4711-ab70-3ccbc78ec9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|unique_email_count|\n",
      "+------------------+\n",
      "|               599|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "unique_emails_df = customer_df.agg(countDistinct(\"email\").alias(\"unique_email_count\"))\n",
    "\n",
    "# show\n",
    "unique_emails_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IIL4RDSCcn4"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iV6PtMZM2si",
    "outputId": "ee8a386b-24f3-4041-ba4c-7cdaf1c30124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# get number of partitions\n",
    "num_partitions = unique_emails_df.rdd.getNumPartitions()\n",
    "\n",
    "print(f\" number of partitions: {num_partitions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_6o4oLIC5SJ"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHntUPHbNBTM",
    "outputId": "2c4eb076-e0e6-48b6-8b3e-326df0c582de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|min_create_date|max_create_date|\n",
      "+---------------+---------------+\n",
      "|     2020-02-14|     2020-02-14|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "create_date_stats = customer_df.agg(\n",
    "    min(\"create_date\").alias(\"min_create_date\"),\n",
    "    max(\"create_date\").alias(\"max_create_date\")\n",
    ")\n",
    "\n",
    "# show\n",
    "create_date_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vndZmoyC-Ay"
   },
   "source": [
    "### Question 6.1\n",
    "\n",
    "Determine which first names occur more than once:\n",
    "\n",
    "1. using the Spark SQL API (printing the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2o1nKhjNHLe",
    "outputId": "4bd0bfa0-445d-4490-e362-79fb78f94f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name|name_count|\n",
      "+----------+----------+\n",
      "|     TERRY|         2|\n",
      "|    WILLIE|         2|\n",
      "|    MARION|         2|\n",
      "|     KELLY|         2|\n",
      "|    LESLIE|         2|\n",
      "|     JAMIE|         2|\n",
      "|     TRACY|         2|\n",
      "|    JESSIE|         2|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use spark sql to find duplicate first names\n",
    "duplicate_first_names_sql = spark.sql(\"\"\"\n",
    "    SELECT first_name, COUNT(*) AS name_count\n",
    "    FROM customer\n",
    "    GROUP BY first_name\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY name_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# show\n",
    "duplicate_first_names_sql.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-qGmjBqDErO"
   },
   "source": [
    "### Question 6.2\n",
    "\n",
    "  2. using the Spark Dataframe API (printing the result once more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqZNDxZnNOZ-",
    "outputId": "de64dd72-9d03-4aec-ab89-2253ad085290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|first_name|count|\n",
      "+----------+-----+\n",
      "|     TERRY|    2|\n",
      "|    WILLIE|    2|\n",
      "|    MARION|    2|\n",
      "|     KELLY|    2|\n",
      "|    LESLIE|    2|\n",
      "|     JAMIE|    2|\n",
      "|     TRACY|    2|\n",
      "|    JESSIE|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "duplicate_first_names_df = customer_df.groupBy(\"first_name\") \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "# show\n",
    "duplicate_first_names_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA56WFXXDqrm"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL):\n",
    "\n",
    "```\n",
    "SELECT\n",
    "   staff.first_name\n",
    "   ,staff.last_name\n",
    "   ,SUM(payment.amount)\n",
    " FROM payment\n",
    "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
    " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2007-02-01'\n",
    " GROUP BY\n",
    "   staff.last_name\n",
    "   ,staff.first_name\n",
    " ORDER BY SUM(payment.amount)\n",
    " ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7igIScORNYDa",
    "outputId": "5a538002-9928-409e-8d57-a6d567ff3fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|last_name|first_name|total_amount|\n",
      "+---------+----------+------------+\n",
      "+---------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read payment table\n",
    "payment_df = spark.read.jdbc(\n",
    "    url=jdbcUrl,\n",
    "    table=\"payment\",\n",
    "    properties=connectionProperties\n",
    ")\n",
    "\n",
    "# read staff table\n",
    "staff_df = spark.read.jdbc(\n",
    "    url=jdbcUrl,\n",
    "    table=\"staff\",\n",
    "    properties=connectionProperties\n",
    ")\n",
    "\n",
    "# convert payment_date to timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "payment_df = payment_df.withColumn(\n",
    "    \"payment_date\",\n",
    "    to_timestamp(\"payment_date\")\n",
    ")\n",
    "\n",
    "# filter payment_date\n",
    "filtered_payment_df = payment_df.filter(\n",
    "    (col(\"payment_date\") >= '2007-01-01') & (col(\"payment_date\") <= '2007-02-01')\n",
    ")\n",
    "\n",
    "# join payment and staff dataframes\n",
    "joined_df = filtered_payment_df.join(staff_df, on=\"staff_id\", how=\"inner\")\n",
    "\n",
    "# group by staff last_name and first_name and compute sum of amount\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "result_df = joined_df.groupBy(\"last_name\", \"first_name\") \\\n",
    "    .agg(_sum(\"amount\").alias(\"total_amount\")) \\\n",
    "    .orderBy(\"total_amount\")\n",
    "\n",
    "# show result\n",
    "# Note no results since there are no records in between the dates of 2007-01-01 and 2007-02-01\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqv7FoidJiBJ"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFCiU2fFNhBX",
    "outputId": "203c51d4-c501-4e27-f923-2e8b3dc10cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor ID: driver\n"
     ]
    }
   ],
   "source": [
    "executor_id = spark.sparkContext.getConf().get(\"spark.executor.id\")\n",
    "print(f\"Executor ID: {executor_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ3a4JB4QVCa"
   },
   "source": [
    " We are executing commands on the driver node. This is because Spark is running in local mode, and there are no separate worker nodes."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
