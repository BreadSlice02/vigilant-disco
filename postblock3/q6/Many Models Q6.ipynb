{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demand Forecasting using PySpark and Prophet"
      ],
      "metadata": {
        "id": "rrVeFczlBSfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "cnTHsc2qBgMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Ingest Data from Source"
      ],
      "metadata": {
        "id": "qxfCPcFNBZsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "jVEy9Gv1BpAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpGSURTO6Zb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d5789c-b748-4edd-e142-ba10da394dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from prophet) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.59)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from prophet) (4.66.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet) (6.4.5)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays<1,>=0.25->prophet) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->prophet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->prophet) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Install Prophet\n",
        "!pip install prophet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Environment Variables and Import Libraries"
      ],
      "metadata": {
        "id": "NZGgrjk4CAjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import logging\n",
        "logging.getLogger('py4j').setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "8J9-oHLq6uKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create SparkSession\n",
        "\n"
      ],
      "metadata": {
        "id": "sOfniGBAB2kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('DemandForecasting') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "zUnHvMqX6yKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and Read Data\n"
      ],
      "metadata": {
        "id": "xT_GkZBjB5K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "!wget https://storage.googleapis.com/bdt-demand-forecast/sales-data.csv\n",
        "# Read the CSV file into a Spark DataFrame\n",
        "df = spark.read.csv('sales-data.csv', header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvfuwlda62m1",
        "outputId": "f1ca6c8f-9f0e-447b-e17c-b7a567cfb223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-27 12:20:17--  https://storage.googleapis.com/bdt-demand-forecast/sales-data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.143.207, 173.194.69.207, 173.194.79.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.143.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17333449 (17M) [text/csv]\n",
            "Saving to: ‘sales-data.csv’\n",
            "\n",
            "\rsales-data.csv        0%[                    ]       0  --.-KB/s               \rsales-data.csv      100%[===================>]  16.53M   110MB/s    in 0.2s    \n",
            "\n",
            "2024-10-27 12:20:18 (110 MB/s) - ‘sales-data.csv’ saved [17333449/17333449]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Data Schema\n"
      ],
      "metadata": {
        "id": "tTDE6VhyB-NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuRYkVzv67n-",
        "outputId": "e55632c0-3d31-4b59-9c50-d6e188e4fff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- store: integer (nullable = true)\n",
            " |-- item: integer (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Date Column to DateType\n"
      ],
      "metadata": {
        "id": "m4vHcTZ6CPW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df = df.withColumn('date', to_date('date', 'yyyy-MM-dd'))\n"
      ],
      "metadata": {
        "id": "koQi40Lz6_bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Data Schema"
      ],
      "metadata": {
        "id": "7MuZQduICSF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlkEUeVz7B6b",
        "outputId": "0be183d2-3524-4fb1-bc9c-6fed9e88983d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- store: integer (nullable = true)\n",
            " |-- item: integer (nullable = true)\n",
            " |-- sales: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView('sales_data')\n"
      ],
      "metadata": {
        "id": "uP1vtwsO7Lj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data by Partitioning\n"
      ],
      "metadata": {
        "id": "Mkqr4Y49CWro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql_statement = '''\n",
        "  SELECT\n",
        "    store,\n",
        "    item,\n",
        "    CAST(date as date) as ds,\n",
        "    SUM(sales) as y\n",
        "  FROM sales_data\n",
        "  GROUP BY store, item, ds\n",
        "  ORDER BY store, item, ds\n",
        "  '''\n",
        "\n",
        "store_item_history = spark.sql(sql_statement)\n"
      ],
      "metadata": {
        "id": "JmTX-YMe7Psm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate Data at Store-Item-Date Level\n"
      ],
      "metadata": {
        "id": "FWY5TYxHCf1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the default parallelism\n",
        "default_parallelism = sc.defaultParallelism\n",
        "print(\"Default parallelism (number of partitions to use):\", default_parallelism)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NWEKspG7RyM",
        "outputId": "a5b3be32-892a-4663-e140-d8f51b6ba0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default parallelism (number of partitions to use): 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repartition the DataFrame\n"
      ],
      "metadata": {
        "id": "cOnjtU53Cigx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition the DataFrame\n",
        "store_item_history = store_item_history.repartition(default_parallelism, ['store', 'item']).cache()\n"
      ],
      "metadata": {
        "id": "JOkRZYCM7Wsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Number of Partitions"
      ],
      "metadata": {
        "id": "dlXLNovWCmZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_partitions = store_item_history.rdd.getNumPartitions()\n",
        "print(\"Number of partitions:\", num_partitions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8OqQHa7aV4",
        "outputId": "bd4bf10e-631a-4551-9d4c-a8d827940797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Model Fit and Forecast to Each Store-Item Combination\n"
      ],
      "metadata": {
        "id": "bT3VLKLJDAOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Schema for Forecast Output"
      ],
      "metadata": {
        "id": "__BSdW6QDFa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_schema =StructType([\n",
        "  StructField('ds',DateType()),\n",
        "  StructField('store',IntegerType()),\n",
        "  StructField('item',IntegerType()),\n",
        "  StructField('y',FloatType()),\n",
        "  StructField('yhat',FloatType()),\n",
        "  StructField('yhat_upper',FloatType()),\n",
        "  StructField('yhat_lower',FloatType())\n",
        "  ])\n"
      ],
      "metadata": {
        "id": "_knH5yCS7eob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the forecast_store_item Function\n"
      ],
      "metadata": {
        "id": "jwtZg-mUDIP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_store_item( history_pd ):\n",
        "\n",
        "  from prophet import Prophet\n",
        "\n",
        "  # Remove missing values\n",
        "  history_pd = history_pd.dropna()\n",
        "\n",
        "  # Configure the model\n",
        "  model = Prophet(\n",
        "    interval_width=0.95,\n",
        "    growth='linear',\n",
        "    daily_seasonality=False,\n",
        "    weekly_seasonality=True,\n",
        "    yearly_seasonality=True,\n",
        "    seasonality_mode='multiplicative'\n",
        "    )\n",
        "\n",
        "  # Train the model\n",
        "  model.fit( history_pd )\n",
        "\n",
        "  # Make predictions\n",
        "  future_pd = model.make_future_dataframe(\n",
        "    periods=90,\n",
        "    freq='D',\n",
        "    include_history=True\n",
        "    )\n",
        "  forecast_pd = model.predict( future_pd )\n",
        "\n",
        "  # Get relevant fields from forecast\n",
        "  f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
        "\n",
        "  # Get relevant fields from history\n",
        "  h_pd = history_pd[['ds','store','item','y']].set_index('ds')\n",
        "\n",
        "  # Join history and forecast\n",
        "  results_pd = f_pd.join( h_pd, how='left' )\n",
        "  results_pd.reset_index(level=0, inplace=True)\n",
        "\n",
        "  # Get store & item from incoming data set\n",
        "  results_pd['store'] = history_pd['store'].iloc[0]\n",
        "  results_pd['item'] = history_pd['item'].iloc[0]\n",
        "\n",
        "  # Return expected dataset\n",
        "  return results_pd[ ['ds', 'store', 'item', 'y', 'yhat', 'yhat_upper', 'yhat_lower'] ]\n"
      ],
      "metadata": {
        "id": "jERQAiYl7gvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the Forecast Function to Each Group\n"
      ],
      "metadata": {
        "id": "VJ5NkZNTDK1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date\n",
        "\n",
        "results = (\n",
        "  store_item_history\n",
        "    .groupBy('store', 'item')\n",
        "      .applyInPandas(forecast_store_item, schema=result_schema)\n",
        "    .withColumn('training_date', current_date() )\n",
        "    )\n"
      ],
      "metadata": {
        "id": "5kVe0VD27k1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persist the Forecasts for Evaluation\n"
      ],
      "metadata": {
        "id": "0QjE2jYoDNyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the forecasts to the local filesystem\n",
        "results.write.csv('forecasts.csv', header=True, mode='overwrite')\n"
      ],
      "metadata": {
        "id": "-abHbQuJ7n2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the Model Evaluation Function to Each Model Result\n"
      ],
      "metadata": {
        "id": "pnNIXrBiDRpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the evaluate_forecast Function"
      ],
      "metadata": {
        "id": "JRQvEttgDUl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_forecast( evaluation_pd ):\n",
        "\n",
        "  from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "  from math import sqrt\n",
        "\n",
        "  # Calculate evaluation metrics\n",
        "  mae = mean_absolute_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
        "  mse = mean_squared_error( evaluation_pd['y'], evaluation_pd['yhat'] )\n",
        "  rmse = sqrt( mse )\n",
        "\n",
        "  # Assemble result set\n",
        "  results = {'store':[evaluation_pd['store'].iloc[0]], 'item':[evaluation_pd['item'].iloc[0]], 'mae':[mae], 'mse':[mse], 'rmse':[rmse]}\n",
        "  return pd.DataFrame( results )\n"
      ],
      "metadata": {
        "id": "ON_oVvJY7q1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the Evaluation Function\n"
      ],
      "metadata": {
        "id": "6UOzMgMvDXLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = (\n",
        "  results\n",
        "    .filter(F.col('ds') < F.lit('2018-01-01')) # Limit evaluation to periods where we have historical data\n",
        "    .groupBy('store', 'item')\n",
        "    .applyInPandas(evaluate_forecast, schema='store int, item int, mae double, mse double, rmse double')\n",
        "    )\n"
      ],
      "metadata": {
        "id": "czKXFXVU7tS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the Evaluation Results\n"
      ],
      "metadata": {
        "id": "3jBaJBBVDdXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ9Qr0HP7vCy",
        "outputId": "c2767f7e-80d0-445a-b04a-c0d217f1c1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------------------+------------------+------------------+\n",
            "|store|item|               mae|               mse|              rmse|\n",
            "+-----+----+------------------+------------------+------------------+\n",
            "|    1|   1| 3.486720561981201|19.388858795166016|4.4032781873470155|\n",
            "|    1|   2| 6.057506084442139| 58.63664627075195| 7.657456906228853|\n",
            "|    1|   3| 4.644035339355469| 34.85238265991211| 5.903590658227594|\n",
            "|    1|   4|  3.64391827583313| 20.51084327697754| 4.528889850391323|\n",
            "|    1|   7| 6.077047348022461| 58.45271301269531| 7.645437398389664|\n",
            "|    1|   8|7.0304155349731445| 77.56575775146484| 8.807142428249065|\n",
            "|    1|   9| 5.609813213348389| 49.88712692260742|7.0630819705428465|\n",
            "|    1|  11| 6.539265155792236| 67.67538452148438| 8.226505000392596|\n",
            "|    1|  16| 3.902801990509033|24.225801467895508| 4.921971298971125|\n",
            "|    1|  19| 5.102372646331787| 40.71337127685547| 6.380703039388016|\n",
            "|    1|  20| 5.218245029449463| 43.94417953491211| 6.629040619494808|\n",
            "|    1|  22|      7.3662109375| 85.57677459716797| 9.250771567667638|\n",
            "|    1|  23|  4.30027437210083|29.537878036499023|5.4348760829018925|\n",
            "|    1|  25| 6.954590797424316| 75.84445190429688| 8.708872022500783|\n",
            "|    1|  26| 5.346170425415039|45.056365966796875| 6.712403888831249|\n",
            "|    1|  27| 3.607870101928711|20.264923095703125|    4.501657816372|\n",
            "|    1|  32| 5.216545104980469|  42.8662223815918| 6.547230130489671|\n",
            "|    1|  33| 6.586278915405273| 68.23731231689453|  8.26058789172384|\n",
            "|    1|  36| 6.834455966949463| 74.78231048583984| 8.647676594660547|\n",
            "|    1|  37| 4.254373550415039|28.501684188842773| 5.338696862422774|\n",
            "+-----+----+------------------+------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Parallelise the Workload\n"
      ],
      "metadata": {
        "id": "tYRtUrWgDhZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrate Parallel Execution\n"
      ],
      "metadata": {
        "id": "GzcEs_0YDrYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import time to measure execution time\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Re-run the forecast application to measure time\n",
        "results = (\n",
        "  store_item_history\n",
        "    .groupBy('store', 'item')\n",
        "      .applyInPandas(forecast_store_item, schema=result_schema)\n",
        "    .withColumn('training_date', current_date() )\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Execution time:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "# Not shown here but it executes in a different (faster time), showcasing the parallelisation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MKvIfdb71P7",
        "outputId": "d1c4d67e-5bd8-4322-dc12-9feeea030467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 0.06285619735717773 seconds\n"
          ]
        }
      ]
    }
  ]
}